{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab1-2-cifar10-exercise-student.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"GVYg9WI5Cmww","colab_type":"text"},"cell_type":"markdown","source":["# Import library"]},{"metadata":{"id":"JeWLqNKXCmw4","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from keras.datasets import cifar10\n","from keras.models import Sequential, Model\n","from keras.layers.core import Dense, Dropout, Activation, Flatten\n","from keras.layers import Conv2D, MaxPooling2D, Input\n","from keras.utils import np_utils\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.callbacks import EarlyStopping\n","\n","from sklearn.metrics import confusion_matrix\n","import itertools\n","\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nE0PkGNmCmxT","colab_type":"text"},"cell_type":"markdown","source":["# Load data"]},{"metadata":{"id":"i7rF7AmpCmxW","colab_type":"code","colab":{}},"cell_type":"code","source":["nb_classes = 10\n","\n","(X_train, y_train), (X_test, y_test) = cifar10.load_data()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PkbBDdfLCmxk","colab_type":"text"},"cell_type":"markdown","source":["# Normalization and pre-processing\n","    The data is loaded as integers, so we must cast it to floating values in order to perfom the division then convert to one hot encoding.\n","    1. use astype('TYPE')\n","    2. use np_utils.to_categorical(data , num_classes)"]},{"metadata":{"id":"GLdZ3_KiCmxm","colab_type":"code","colab":{}},"cell_type":"code","source":["### TO DO"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5fSJ4EWwCmx5","colab_type":"text"},"cell_type":"markdown","source":["# Build CNN Model\n","\n","We need to specify the shape of the input which is (32, 32, 3).\n","\n","The first layer and second layer is the Conv2d layer. We need 32 filters and kernel size = 3 and used ReLU (rectified linear unit) as our activation function. \n","\n","The third layer is the MaxPooling layer that has pool size (2,2)\n","\n","The fourth layer is Dropout layer will randomly disable 20% of the neurons.\n","\n","The fifth and sixth layer are same first and second layer but increase filters from 32 to 64.\n","\n","The seventh are same thrid layer.\n","\n","The eighth is Dropout layer will randomly 25% of the neurons.\n","\n","After creating all the convolutional layers, we need to flatten them, so that they can act as an input to the Dense layers.\n","\n","Dense layers have 512 neurons and activation function is 'relu'.\n","\n","Dropout will randomnly disable 50% of the neurons.\n","\n","The last layer is the Dense layer with 10 neurons and activation function is Softmax . \n","\n","Our model have architecture belowing:"]},{"metadata":{"id":"HSxUMko8CmyC","colab_type":"text"},"cell_type":"markdown","source":["![](https://pbs.twimg.com/media/DJDu-seXcAAvqZf.jpg:large)\n","\n","*(Source: https://twitter.com/DeepLearn007/status/905486345862344704)*"]},{"metadata":{"id":"778TABQlCmyF","colab_type":"text"},"cell_type":"markdown","source":["Function :\n","    1. Input(shape = (input_shape))\n","    2. Conv2D(filters, kernel_size, activation = 'activation')(previous_layer)\n","    3. MaxPooling2D(pool_size)(previous_layer)\n","    4. Dropout(rate)(previous_layer)\n","    5. Flatten()(previous_layer)\n","    6. Dense(units , activation = 'activation')(previous_layer)\n","    7. Model(inputs , outputs)"]},{"metadata":{"id":"OC_b0OZeCmyJ","colab_type":"text"},"cell_type":"markdown","source":["If you create model success your model must have model.summary() like below :"]},{"metadata":{"id":"5YKyxZsYCmyL","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","```\n","# This is formatted as code\n","```\n","\n","\n","<!-- cnn_sol.png -->\n","![](https://i.imgur.com/Y4jveXj.png)"]},{"metadata":{"id":"S1iyLYFICmyO","colab_type":"code","colab":{}},"cell_type":"code","source":["### TO DO"],"execution_count":0,"outputs":[]},{"metadata":{"id":"v1VWU05wCmyV","colab_type":"text"},"cell_type":"markdown","source":["# Compile\n","we use loss function categorical crossentropy and optimizer adam to compile our model.\n","    \n","function = compile(loss='loss_function', optimizer='optimizer_functin',metrics=['accuracy'])"]},{"metadata":{"id":"9wf9C6TDCmyX","colab_type":"code","colab":{}},"cell_type":"code","source":["### TO DO"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LKrYchSSCmyd","colab_type":"text"},"cell_type":"markdown","source":["# Fit data\n","If you create function fit data already. "]},{"metadata":{"id":"4IoRQmMhCmyf","colab_type":"code","colab":{}},"cell_type":"code","source":["### TO DO"],"execution_count":0,"outputs":[]},{"metadata":{"id":"k2co6UGdCmzE","colab_type":"text"},"cell_type":"markdown","source":["# Evaluate\n","function = evaluate(Test_data, verbose=0)"]},{"metadata":{"id":"7ENzo32DCmzF","colab_type":"code","colab":{}},"cell_type":"code","source":["### TO DO"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Q9QhW66eCmzi","colab_type":"text"},"cell_type":"markdown","source":["# Plot show acc and loss"]},{"metadata":{"id":"VBlQ8n7aCmzm","colab_type":"code","colab":{}},"cell_type":"code","source":["plt.rcParams['figure.figsize'] = (5,5)  \n","plt.figure(0)\n","plt.plot(history.history['loss'],'ro')\n","plt.plot(history.history['val_loss'],'b+')\n","plt.xticks(np.arange(0, 20, 2.0))\n","plt.rcParams['figure.figsize'] = (8, 6)\n","plt.xlabel(\"Num of Epochs\")\n","plt.ylabel(\"Loss\")\n","plt.title(\"Training Loss vs Validation Loss\")\n","plt.legend(['train','validation'])\n","\n","plt.rcParams['figure.figsize'] = (5,5) \n","plt.figure(1)\n","plt.plot(history.history['acc'],'ro')\n","plt.plot(history.history['val_acc'],'b+')\n","plt.xticks(np.arange(0, 20, 2.0))\n","plt.rcParams['figure.figsize'] = (8, 6)\n","plt.xlabel(\"Num of Epochs\")\n","plt.ylabel(\"Accuracy\")\n","plt.title(\"Training Accuracy vs Validation Accuracy\")\n","plt.legend(['train','validation'])\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mpWP0n8xCmzq","colab_type":"text"},"cell_type":"markdown","source":["# Confusion matrix"]},{"metadata":{"id":"ARalpFw0Cmz3","colab_type":"code","colab":{}},"cell_type":"code","source":["def plot_confusion_matrix(cm, classes, title='Confusion matrix',cmap=plt.cm.Blues):\n","\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes)\n","    plt.yticks(tick_marks, classes)\n","\n","    thresh = cm.max() / 2.\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        plt.text(j, i, cm[i, j],\n","                 horizontalalignment=\"center\",\n","                 color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","    plt.tight_layout()\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","\n","# Predict the values from the validation dataset\n","y_pred = model.predict(X_test)\n","# Convert predictions classes to one hot vectors \n","y_pred_classes = np.argmax(y_pred, axis = 1) \n","# Convert validation observations to one hot vectors\n","y_true = np.argmax(y_test, axis = 1) \n","# compute the confusion matrix\n","confusion_mtx = confusion_matrix(y_true, y_pred_classes) \n","# plot the confusion matrix\n","plot_confusion_matrix(confusion_mtx, classes = range(10))"],"execution_count":0,"outputs":[]},{"metadata":{"collapsed":true,"id":"r1GvHBwWCm0m","colab_type":"text"},"cell_type":"markdown","source":["# Inspecting output"]},{"metadata":{"id":"fE6Qul72Cm0n","colab_type":"code","colab":{}},"cell_type":"code","source":["predicted = model.predict(X_test)\n","predicted_classes = np.argmax(predicted,axis=1)\n","correct_indices = np.nonzero(predicted_classes == y_test)[0]\n","incorrect_indices = np.nonzero(predicted_classes != y_test)[0]\n","plt.rcParams['figure.figsize'] = (7,7) \n","plt.figure()\n","for i, correct in enumerate(correct_indices[:1]):\n","    plt.subplot(3,3,i+1)\n","    plt.imshow(X_test[correct], cmap='viridis', interpolation='none')\n","    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[correct], y_test[correct]))\n","    \n","plt.figure()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wBPJLGteCm0q","colab_type":"text"},"cell_type":"markdown","source":["# Data augmentation"]},{"metadata":{"id":"Jpwat44HCm0r","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras.preprocessing.image import ImageDataGenerator"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yi9nLMWVCm0x","colab_type":"text"},"cell_type":"markdown","source":["Link : https://keras.io/preprocessing/image/"]},{"metadata":{"id":"RQA3C1KrCm0z","colab_type":"text"},"cell_type":"markdown","source":["# Inspecting data"]},{"metadata":{"id":"L7v1IAOsCm03","colab_type":"code","colab":{}},"cell_type":"code","source":["for i in range(0, 9):\n","    plt.subplot(330 + 1 + i)\n","    plt.imshow(X_train[i], cmap=plt.get_cmap('gray'))\n","    # show the plot\n","    plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DTHmtwWqCm06","colab_type":"code","colab":{}},"cell_type":"code","source":["datagen = ImageDataGenerator(\n","            featurewise_center=False,  # set input mean to 0 over the dataset\n","            samplewise_center=False,  # set each sample mean to 0\n","            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n","            samplewise_std_normalization=False,  # divide each input by its std\n","            zca_whitening=False,  # apply ZCA whitening\n","            rotation_range=False,  # randomly rotate images in the range (degrees, 0 to 180)\n","            width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n","            height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n","            horizontal_flip=True,  # randomly flip images\n","            vertical_flip=False)  # randomly flip images"],"execution_count":0,"outputs":[]},{"metadata":{"id":"y0MNKX5fCm2S","colab_type":"text"},"cell_type":"markdown","source":["## Fit data to datagen\n","\n","function -> datagen.fit(data)\n","\n","## Fit data to the same model\n","\n","function -> model.fit_generator(datagen.flow(train_data , batch_size = batch_size), epochs =nb_classes,\n","            verbose=1,validation_data=(test_data))"]},{"metadata":{"id":"Hh_VzEZdCm2T","colab_type":"code","colab":{}},"cell_type":"code","source":["### To Do"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7GCvdm16Cm2f","colab_type":"text"},"cell_type":"markdown","source":["# Inspecting data augmentation"]},{"metadata":{"id":"FCx0gkbZCm2h","colab_type":"code","colab":{}},"cell_type":"code","source":["for X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=128):\n","    for i in range(0, 9):\n","        plt.subplot(330 + 1 + i)\n","        plt.imshow(X_batch[i], cmap=plt.get_cmap('gray'))\n","    # show the plot\n","    plt.show()\n","    break"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ObD1TkRnCm2t","colab_type":"text"},"cell_type":"markdown","source":["# Evaluate"]},{"metadata":{"id":"80oIZi-QCm2u","colab_type":"code","colab":{}},"cell_type":"code","source":["### To Do "],"execution_count":0,"outputs":[]}]}